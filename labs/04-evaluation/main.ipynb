{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup and Imports\n",
    "Import required libraries and set up environment variables using dotenv. Configure notebook directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup and Imports\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation import (\n",
    "    evaluate, \n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator, \n",
    "    GroundednessEvaluator,\n",
    "    F1ScoreEvaluator\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure notebook directory paths\n",
    "notebook_dir = os.path.abspath(os.path.dirname(''))\n",
    "eval_path = os.path.join(notebook_dir, 'evals.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for File Operations\n",
    "Define functions for file handling including get_latest_eval_file() with notebook-specific path handling using os.path.abspath(os.path.dirname(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for File Operations\n",
    "\n",
    "def get_latest_eval_file() -> str:\n",
    "    \"\"\"Get the most recent evaluation results file.\"\"\"\n",
    "    eval_dir = os.path.join(notebook_dir, 'eval_results')\n",
    "    print (f\"Looking for evaluation results in {eval_dir}\")\n",
    "    files = glob.glob(os.path.join(eval_dir, 'eval_results_*.jsonl'))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No evaluation result files found\")\n",
    "    \n",
    "    # Sort by modification time\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    return latest_file\n",
    "\n",
    "def validate_record(record: Dict) -> bool:\n",
    "    \"\"\"Check if a record has all required fields and valid data.\"\"\"\n",
    "    required_fields = ['question_id', 'question', 'ground_truth', \n",
    "                      'kyc_context', 'response', 'timestamp']\n",
    "    \n",
    "    # Check if all required fields exist\n",
    "    if not all(field in record for field in required_fields):\n",
    "        return False\n",
    "    \n",
    "    # Check if kyc_context is valid JSON\n",
    "    if record['kyc_context']:\n",
    "        try:\n",
    "            json.loads(record['kyc_context'])\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    # Check if response is not empty\n",
    "    if not record['response'].strip():\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def load_and_validate_results(filepath: str) -> List[Dict]:\n",
    "    \"\"\"Load and validate evaluation results from file.\"\"\"\n",
    "    valid_records = []\n",
    "    total_records = 0\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            total_records += 1\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                if validate_record(record):\n",
    "                    valid_records.append(record)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nProcessed {total_records} total records\")\n",
    "    print(f\"Found {len(valid_records)} valid records\")\n",
    "    print(f\"Filtered out {total_records - len(valid_records)} invalid records\")\n",
    "    \n",
    "    return valid_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation Functions\n",
    "Implement validate_record() and load_and_validate_results() functions for data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Validation Functions\n",
    "\n",
    "def validate_record(record: Dict) -> bool:\n",
    "    \"\"\"Check if a record has all required fields and valid data.\"\"\"\n",
    "    required_fields = ['question_id', 'question', 'ground_truth', \n",
    "                      'kyc_context', 'response', 'timestamp']\n",
    "    \n",
    "    # Check if all required fields exist\n",
    "    if not all(field in record for field in required_fields):\n",
    "        return False\n",
    "    \n",
    "    # Check if kyc_context is valid JSON\n",
    "    if record['kyc_context']:\n",
    "        try:\n",
    "            json.loads(record['kyc_context'])\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    # Check if response is not empty\n",
    "    if not record['response'].strip():\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def load_and_validate_results(filepath: str) -> List[Dict]:\n",
    "    \"\"\"Load and validate evaluation results from file.\"\"\"\n",
    "    valid_records = []\n",
    "    total_records = 0\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            total_records += 1\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                if validate_record(record):\n",
    "                    valid_records.append(record)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nProcessed {total_records} total records\")\n",
    "    print(f\"Found {len(valid_records)} valid records\")\n",
    "    print(f\"Filtered out {total_records - len(valid_records)} invalid records\")\n",
    "    \n",
    "    return valid_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Configuration\n",
    "Set up validate_environment() function and configure required Azure environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Configuration\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"Validate required environment variables are set.\"\"\"\n",
    "    required_vars = {\n",
    "        \"AZURE_OPENAI_ENDPOINT\": \"Azure OpenAI endpoint URL\",\n",
    "        \"AZURE_OPENAI_API_KEY\": \"Azure OpenAI API key\",\n",
    "        \"AZURE_OPENAI_DEPLOYMENT\": \"Azure OpenAI deployment name\",\n",
    "        \"AZURE_OPENAI_API_VERSION\": \"Azure OpenAI API version\",\n",
    "        \"AZURE_SUBSCRIPTION_ID\": \"Azure subscription ID\",\n",
    "        \"AZURE_RESOURCE_GROUP\": \"Azure resource group name\",\n",
    "        \"AZURE_PROJECT_NAME\": \"Azure AI project name\"\n",
    "    }\n",
    "    \n",
    "    missing_vars = []\n",
    "    for var, description in required_vars.items():\n",
    "        if not os.environ.get(var):\n",
    "            missing_vars.append(f\"{var} ({description})\")\n",
    "    \n",
    "    if missing_vars:\n",
    "        raise ValueError(\n",
    "            \"Missing required environment variables:\\n\" + \n",
    "            \"\\n\".join(f\"- {var}\" for var in missing_vars)\n",
    "        )\n",
    "\n",
    "# Validate environment variables\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Setup\n",
    "Initialize evaluators with model configuration and set up Azure AI project details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation Setup\n",
    "\n",
    "# Load environment variables for model configuration\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    \"api_key\": os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    \"azure_deployment\": os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    "    \"api_version\": os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "}\n",
    "\n",
    "# Load environment variables for Azure AI project details\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "    \"resource_group_name\": os.environ[\"AZURE_RESOURCE_GROUP\"],\n",
    "    \"project_name\": os.environ[\"AZURE_PROJECT_NAME\"],\n",
    "}\n",
    "\n",
    "# Initialize evaluators with model configuration\n",
    "evaluators = {\n",
    "    \"relevance\": RelevanceEvaluator(model_config=model_config),\n",
    "    \"coherence\": CoherenceEvaluator(model_config=model_config),\n",
    "    \"groundedness\": GroundednessEvaluator(model_config=model_config),\n",
    "    \"f1_score\": F1ScoreEvaluator()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Evaluation\n",
    "Execute the evaluation process and display results using the evaluate() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation\n",
    "\n",
    "# Get the latest evaluation file\n",
    "latest_file = get_latest_eval_file()\n",
    "print(f\"Processing file: {os.path.basename(latest_file)}\")\n",
    "\n",
    "# Run evaluations using evaluate() function\n",
    "result = evaluate(\n",
    "    data=latest_file,\n",
    "    evaluators=evaluators,\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.question}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"context\": \"${data.kyc_context}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,\n",
    "    output_path=\"./eval_metrics.json\"\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Evaluation Simulators Overview\n",
    "\n",
    "## Available Scenarios\n",
    "\n",
    "| Simulator Type | Max Simulations | Purpose | Turns |\n",
    "|---------------|-----------------|----------|-------|\n",
    "| Question Answering (`ADVERSARIAL_QA`) | 1,384 | Content safety evaluation | Single |\n",
    "| Conversation (`ADVERSARIAL_CONVERSATION`) | 1,018 | Content safety evaluation | Multi |\n",
    "| Summarization (`ADVERSARIAL_SUMMARIZATION`) | 525 | Content safety evaluation | Single |\n",
    "| Search (`ADVERSARIAL_SEARCH`) | 1,000 | Content safety evaluation | Single |\n",
    "| Text Rewrite (`ADVERSARIAL_REWRITE`) | 1,000 | Content safety evaluation | Single |\n",
    "| Ungrounded Content Gen (`ADVERSARIAL_CONTENT_GEN_UNGROUNDED`) | 496 | Content safety evaluation | Single |\n",
    "| Grounded Content Gen (`ADVERSARIAL_CONTENT_GEN_GROUNDED`) | 475 | Content safety & UPIA jailbreak | Single |\n",
    "| Protected Material (`ADVERSARIAL_PROTECTED_MATERIAL`) | 306 | Protected material evaluation | Single |\n",
    "\n",
    "## Evaluation Types\n",
    "\n",
    "- **Content Safety**: All simulators can evaluate for:\n",
    "  - Hateful/unfair content\n",
    "  - Sexual content  \n",
    "  - Violent content\n",
    "  - Self-harm content\n",
    "\n",
    "- **Jailbreak Testing**:\n",
    "  - Direct attacks (UPIA) using `DirectAttackSimulator`\n",
    "  - Indirect attacks (XPIA) using `IndirectAttackSimulator`\n",
    "\n",
    "The simulators help assess model safety and robustness against various adversarial inputs and attack vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation.simulator import AdversarialScenario, DirectAttackSimulator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "# Create a list to store all generated queries\n",
    "generated_queries = []\n",
    "\n",
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Callback function to collect generated queries\n",
    "    \"\"\"\n",
    "    # Extract the query from the first message\n",
    "    query = messages[\"messages\"][0][\"content\"]\n",
    "    \n",
    "    # Add query to our collection\n",
    "    generated_queries.append(query)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"],\n",
    "        \"stream\": stream,\n",
    "        \"session_state\": session_state\n",
    "    }\n",
    "\n",
    "# Configure Azure AI project details\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "    \"resource_group_name\": os.environ[\"AZURE_RESOURCE_GROUP\"],\n",
    "    \"project_name\": os.environ[\"AZURE_PROJECT_NAME\"]\n",
    "}\n",
    "\n",
    "\n",
    "async def run_adversarial_simulation():\n",
    "    credential = DefaultAzureCredential()\n",
    "    adversarial_simulator = DirectAttackSimulator(\n",
    "        azure_ai_project=azure_ai_project, \n",
    "        credential=credential\n",
    "    )\n",
    "    \n",
    "    outputs = await adversarial_simulator(\n",
    "        scenario=AdversarialScenario.ADVERSARIAL_CONTENT_PROTECTED_MATERIAL,\n",
    "        target=callback,\n",
    "        max_simulation_results=10,\n",
    "        max_conversation_turns=3\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    # Print all collected queries\n",
    "    print(\"\\nGenerated Queries:\")\n",
    "    print(\"-----------------------------\")\n",
    "    for i, query in enumerate(generated_queries, 1):\n",
    "        print(f\"\\n{i}. {query}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Run the simulation in Jupyter\n",
    "await run_adversarial_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
